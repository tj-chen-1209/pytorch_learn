# PyTorch å­¦ä¹ ç¬”è®°

> è®°å½•è‡ªå·±å­¦ä¹  PyTorch çš„è¿‡ç¨‹ã€ä»£ç ç¤ºä¾‹å’Œå¿ƒå¾—ä½“ä¼šã€‚

---

## ğŸ“Œ å­¦ä¹ è®¡åˆ’

- [ ] å®‰è£…ä¸ç¯å¢ƒé…ç½®
- [ ] Tensor åŸºç¡€
- [ ] Autograd è‡ªåŠ¨æ±‚å¯¼
- [ ] ç¥ç»ç½‘ç»œæ„å»º
- [ ] æ¨¡å‹è®­ç»ƒä¸ä¼˜åŒ–
- [ ] æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
- [ ] é¡¹ç›®å®æˆ˜ï¼ˆMNIST / CIFAR-10ï¼‰

---

## ğŸ›  ç¯å¢ƒé…ç½®

```bash
# å»å®˜ç½‘ä¸‹è½½å¹¶å®‰è£…Anaconda (ç‰ˆæœ¬è‡ªå·±å»å®˜ç½‘ä¸‹è½½æ”¹åŠ¨)
wget https://repo.anaconda.com/archive/Anaconda3-2024.02-1-Linux-x86_64.sh

# å®‰è£…
bash Anaconda3-2024.02-1-Linux-x86_64.sh

# åˆ·æ–°ç¯å¢ƒå˜é‡
source ~/.zshrc

# ç”¨ conda åœ¨ base ç¯å¢ƒé‡Œå®‰è£… mamba
conda install -n base -c conda-forge mamba -y

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆmamba æ¨è æé€Ÿï¼‰
mamba create -n test python=3.10 -y

# æ¿€æ´»ç¯å¢ƒ
conda activate test

# å®‰è£… pytorch (æ ¹æ®å®˜ç½‘é€‰æ‹© CUDA/CPU ç‰ˆæœ¬)
mamba install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia -y
```

## ğŸ“š ç›¸å…³å‡½æ•°

- Tensor åˆ›å»ºæ–¹å¼

```python
# unintialized data
x = torch.empty(5, 3)

# random data å‡åŒ€åˆ†å¸ƒ [0, 1ã€‘
x = torch.rand(5, 3) 

# random data æ­£æ€éšæœºåˆ†å¸ƒ
x = torch.randn(5, 3)

x = torch.zeros(5, 3, dtype=torch.long)

x = torch.tensor([5.5, 3])

# new_* methods take in sizes
x = x.new_ones(5, 3, dtype=torch.double)
# override dtype!
x = torch.randn_like(x, dtype=torch.float)
```

- å¼ é‡æ”¹å˜ç»´åº¦

```python
# torch.view( , )
x = torch.randn(4, 4)
y = x.view(16) # 16 * 1
z = x.view(-1, 8) # 2 * 8
```

- åå‘ä¼ æ’­

```python
# required_grad æ˜¯å‘Šè¯‰torchè¦è®°å½•xçš„èŠ‚ç‚¹å›¾
x = torch.ones(2, 2, requires_grad=True)

# grad_fn æ˜¯æŸä¸ªå‰å‘ç®—å­å¯¹åº”çš„åå‘è§„åˆ™ é€šä¿—æ¥è®²å°±æ˜¯ä»–æ˜¯ç”¨ä»€ä¹ˆè®¡ç®—å¾—åˆ°çš„å°±ä¼šè¾“å‡ºä»€ä¹ˆè§„åˆ™
# åŸºæœ¬ç®—å­ï¼š
# AddBackward0ï¼šåŠ æ³•
# SubBackward0ï¼šå‡æ³•
# MulBackward0ï¼šä¹˜æ³•
# DivBackward0ï¼šé™¤æ³•
# PowBackward0ï¼šå¹‚è¿ç®—
# ä¸ºä»€ä¹ˆéƒ½æ˜¯ ...Backward0?  
#PyTorch åœ¨ æ–°ç‰ˆ Autograd å¼•æ“ï¼ˆåŸºäº C++ DifferentiableGraphï¼‰ é‡Œç”Ÿæˆçš„é»˜è®¤åç¼€ã€‚
y = x + 2
print(y.grad_fn)

# å¯¹äºæœ€åç”Ÿæˆçš„æ˜¯æ ‡é‡ï¼š
z = y * 3 * 3
out = z.mean()
out.backward()
# å°±å¯ä»¥éšæ„è¾“å‡ºåœ¨è¿™æ¡ä¼ æ’­è·¯çº¿ä¸Šçš„è¾“å‡ºoutå¯¹ä»»ä½•ä¸€ä¸ªå˜é‡çš„æ¢¯åº¦
x.grad # (dout)/dx

# å¯¹äºç”Ÿäº§æ˜¯å¤šç»´å‘é‡ï¼š
x = torch.tensor([2.0, 3.0], requires_grad=True)
y = x * 2 * 2
v = torch.tensor([1.0, 0.5]) 
y.backward(v) # ç›¸å½“äºå®šä¹‰äº†ä¸€ä¸ªæ–°çš„loss = v_T * y
x.grad # (dloss)/dx

# stop autograd åˆ‡æ–­åé¢æ‰€æœ‰çš„èŠ‚ç‚¹å›¾ ä¸‹é¢è¿™ä¸ªkå°±æ²¡æœ‰backward
x = torch.tensor([2., 2.], requires_grad=True)
with torch.no_grad():
    y = x * 2
    # y.sum().backward() è¿™ä¸ªå°±ä¼šæŠ¥é”™ å› ä¸ºrequired_grad=False

x.sum().backward()
x.grad
```

- Jacobian Matrix:

ç›¸å½“äºå¤šç»´åº¦ä¸Šçš„æ–œç‡çŸ©é˜µï¼Œå¯ä»¥è¿‘ä¼¼å–å‡½æ•°$\delta x$çš„å€¼ï¼Œè®¡ç®—æ–¹æ³•ï¼š

$$
y = f(x) = \begin{bmatrix}
x_1^2 \\
x_2^2
\end{bmatrix}
$$

å®ƒçš„ Jacobian æ˜¯ï¼š

$$
J = \frac{\partial y}{\partial x} =\begin{bmatrix}
2x_1 & 0 \\
0 & 2x_2
\end{bmatrix}
$$

è°ƒç”¨ `y.backward(v)` æ—¶ï¼ŒPyTorch è®¡ç®—çš„æ˜¯ï¼š

$$
\nabla_x (v^\top y) = v^\top J
$$

ä¹Ÿå°±æ˜¯ï¼š

$$
\frac{\partial (v_1 y_1 + v_2 y_2)}{\partial x}
= (2v_1 x_1, \; 2v_2 x_2)
$$

- æœ€å°ç¥ç»ç½‘ç»œæ­å»ºï¼š

```python

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

# å®šä¹‰ç½‘ç»œ


class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 6, 5)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16*5*5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def num_flat_features(self, x):
        size = x.size()[1:]  # é™¤ batch å¤–æ‰€æœ‰ç»´åº¦
        num_features = 1
        for s in size:
            num_features *= s
        return num_features

    def forward(self, x):
        x = F.max_pool2d(F.relu(self.conv1(x)), 2)
        x = F.max_pool2d(F.relu(self.conv2(x)), 2)
        x = x.view(-1, self.num_flat_features(x))  # å±•å¹³
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x


# åˆå§‹åŒ–ç½‘ç»œå’Œä¼˜åŒ–å™¨
net = Net()
criterion = nn.MSELoss()
optimizer = optim.SGD(net.parameters(), lr=0.01)

# ä¸€ä¸ªå°è®­ç»ƒå¾ªç¯
for epoch in range(5):  # è·‘5ä¸ªepoch
    # å‡æ•°æ®ï¼šbatch_size=1ï¼Œ1é€šé“ï¼Œ32x32 å›¾åƒ
    inputs = torch.randn(1, 1, 32, 32)
    targets = torch.randn(1, 10)  # éšæœºç›®æ ‡ï¼Œå½¢çŠ¶ (1,10)

    # â‘  æ¢¯åº¦æ¸…é›¶
    optimizer.zero_grad()

    # â‘¡ å‰å‘ä¼ æ’­
    outputs = net(inputs)

    # â‘¢ è®¡ç®—æŸå¤±
    loss = criterion(outputs, targets)

    # â‘£ åå‘ä¼ æ’­
    loss.backward()

    # â‘¤ æ›´æ–°å‚æ•°
    optimizer.step()

    print(f"Epoch {epoch+1}, Loss: {loss.item():.4f}")
```
